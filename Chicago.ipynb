{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Never a sity so real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "import utils\n",
    "\n",
    "# We use Gemini directly instead through langchain\n",
    "# because of safety settings,\n",
    "# langchain is developing this and not hasn't released yet\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(\n",
    "    api_key=os.environ.get(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "llm_gemini = genai.GenerativeModel(\n",
    "    model_name = \"gemini-pro\",\n",
    "    safety_settings=utils.safety_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Never a City So Real A Walk in Chicago (Alex Kotlowitz) (Z-Library)\"\n",
    "input_file = f\"Chicago/{file_name}.pdf\"\n",
    "output_file = f\"Chicago/{file_name}-cn.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(input_file)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_output = open(output_file, \"a\")\n",
    "\n",
    "for index, page in enumerate(pages[:]):\n",
    "    prompt = f\"\"\" Suppose you are a translator, and your target language is simplified Chinese, \n",
    "    Your audiences are Chinese native young students in Junior school about 13 years old. \n",
    "    Translate the following english text to simplified Chinese: \\ ```{page.page_content}``` \"\"\" \n",
    "    \n",
    "    response = llm_gemini.generate_content(\n",
    "        prompt,\n",
    "        safety_settings=utils.safety_config\n",
    "    )\n",
    "\n",
    "    file_output.write(f\"\\n---- page: {index} ---- \\n\")\n",
    "    if len(response.candidates) > 0:\n",
    "        # we only get the first candidate\n",
    "        for p in response.candidates[0].content.parts:\n",
    "            file_output.write(p.text.strip())\n",
    "        file_output.flush()\n",
    "    \n",
    "    # if len(response.parts) > 0:\n",
    "    #     for p in response.parts:\n",
    "    #         file_output.write(p.text.strip())\n",
    "    #     file_output.flush()\n",
    "\n",
    "file_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/llm-env/llm-jupiter-2/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:79\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/llm-env/llm-jupiter-2/lib/python3.11/site-packages/grpc/_channel.py:1160\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1154\u001b[0m (\n\u001b[1;32m   1155\u001b[0m     state,\n\u001b[1;32m   1156\u001b[0m     call,\n\u001b[1;32m   1157\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1158\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1159\u001b[0m )\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm-env/llm-jupiter-2/lib/python3.11/site-packages/grpc/_channel.py:1003\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:7890 {created_time:\"2024-03-10T10:14:31.791786+08:00\", grpc_status:13, grpc_message:\"An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pages[\u001b[38;5;241m119\u001b[39m:\u001b[38;5;241m123\u001b[39m]):\n\u001b[1;32m      5\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m Suppose you are a translator, and your target language is simplified Chinese, \u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    Your audiences are Chinese native young students in Junior school about 13 years old. \u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m    Translate the following english text to simplified Chinese: \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m ```\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;241m.\u001b[39mpage_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m``` \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m \n\u001b[0;32m----> 9\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_gemini\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafety_config\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m candidate \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcandidates:\n\u001b[1;32m     15\u001b[0m         [part\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m candidate\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mparts]\n",
      "File \u001b[0;32m~/llm-env/llm-jupiter-2/lib/python3.11/site-packages/google/generativeai/generative_models.py:248\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n",
      "File \u001b[0;32m~/llm-env/llm-jupiter-2/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:566\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    561\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m    562\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmodel),)),\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/llm-env/llm-jupiter-2/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm-env/llm-jupiter-2/lib/python3.11/site-packages/google/api_core/retry.py:372\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    371\u001b[0m )\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm-env/llm-jupiter-2/lib/python3.11/site-packages/google/api_core/retry.py:207\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    209\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/llm-env/llm-jupiter-2/lib/python3.11/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm-env/llm-jupiter-2/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:81\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInternalServerError\u001b[0m: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting"
     ]
    }
   ],
   "source": [
    "# test\n",
    "output_test_file = f\"Chicago/test-cn.txt\"\n",
    "output_test = open(output_test_file, \"a\")\n",
    "for index, page in enumerate(pages[119:123]):\n",
    "    prompt = f\"\"\" Suppose you are a translator, and your target language is simplified Chinese, \n",
    "    Your audiences are Chinese native young students in Junior school about 13 years old. \n",
    "    Translate the following english text to simplified Chinese: \\ ```{page.page_content}``` \"\"\" \n",
    "    \n",
    "    response = llm_gemini.generate_content(\n",
    "        prompt,\n",
    "        safety_settings=utils.safety_config\n",
    "    )\n",
    "\n",
    "    for candidate in response.candidates:\n",
    "        [part.text for part in candidate.content.parts]\n",
    "\n",
    "    print(f\"----{index+119}----\")\n",
    "    if len(response.candidates) > 0:\n",
    "        print(f\"----candid----\")\n",
    "        # output_test.write(\"--candidates--\")\n",
    "        # print(response.candidates)\n",
    "        for c in response.candidates:\n",
    "            for p in c.content.parts:\n",
    "                output_test.write(p.text.strip())\n",
    "            output_test.flush()\n",
    "    # if len(response.parts) > 0:\n",
    "    #     print(f\"----normal----\")\n",
    "    #     output_test.write(\"--text--\")\n",
    "    #     for p in response.parts:\n",
    "    #         output_test.write(p.text.strip())\n",
    "    #         output_test.flush()\n",
    "\n",
    "output_test.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bel-llm-jupiter-2",
   "language": "python",
   "name": "bel-llm-jupiter-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
